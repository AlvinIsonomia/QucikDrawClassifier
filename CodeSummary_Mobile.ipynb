{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt # 基础数据处理\n",
    "\n",
    "from random import sample # 随机采样\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader # Pytorch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader & Preprocessing\n",
    "For dataprocessing, the mean and std will be computed when init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading class: airplane in ./train\n",
      "loading class: ant in ./train\n",
      "loading class: bear in ./train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10582 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading class: bird in ./train\n",
      "loading class: bridge in ./train\n",
      "loading class: bus in ./train\n",
      "loading class: calendar in ./train\n",
      "loading class: car in ./train\n",
      "loading class: chair in ./train\n",
      "loading class: dog in ./train\n",
      "loading class: dolphin in ./train\n",
      "loading class: door in ./train\n",
      "loading class: flower in ./train\n",
      "loading class: fork in ./train\n",
      "loading class: truck in ./train\n",
      "loading class: airplane in ./handwashed\n",
      "loading class: ant in ./handwashed\n",
      "loading class: bear in ./handwashed\n",
      "loading class: bird in ./handwashed\n",
      "loading class: bridge in ./handwashed\n",
      "loading class: bus in ./handwashed\n",
      "loading class: calendar in ./handwashed\n",
      "loading class: car in ./handwashed\n",
      "loading class: chair in ./handwashed\n",
      "loading class: dog in ./handwashed\n",
      "loading class: dolphin in ./handwashed\n",
      "loading class: door in ./handwashed\n",
      "loading class: flower in ./handwashed\n",
      "loading class: fork in ./handwashed\n",
      "loading class: truck in ./handwashed\n",
      "Data loading finished, 10582 totally\n",
      "Data Cleaning Fire ON!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10582/10582 [00:01<00:00, 6456.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished, 10161 remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_root = './train'\n",
    "labels = os.listdir(train_root)\n",
    "\n",
    "\n",
    "class DrawDataset(Dataset):\n",
    "    def __init__(self, data_root = '',transform = None,anlysis = False):\n",
    "        self.mean = 0.16443629118313927\n",
    "        self.std = 0.3239202530511322\n",
    "        self.transform = transform\n",
    "        self.anlysis = False # only used when first computation\n",
    "        self.label_mapper = {'airplane': 0, 'ant': 1, 'bear': 2, 'bird': 3, 'bridge': 4,\n",
    "                        'bus'     : 5, 'calendar': 6, 'car': 7, 'chair': 8, 'dog': 9,\n",
    "                        'dolphin' : 10, 'door': 11, 'flower': 12, 'fork': 13, 'truck': 14}\n",
    "        self.IO_mapper = []\n",
    "        for train_root in data_root:\n",
    "            for x in self.label_mapper:\n",
    "                print('loading class:',x,'in',train_root)\n",
    "                temp_root = os.path.join(train_root,x)\n",
    "                temp_list = os.listdir(temp_root)\n",
    "                for img in temp_list:\n",
    "                    full_str = os.path.join(temp_root,img)\n",
    "                    self.IO_mapper.append([full_str , self.label_mapper[x]])\n",
    "        print('Data loading finished,',len(self.IO_mapper),'totally')\n",
    "        print('Data Cleaning Fire ON!!!')\n",
    "        self.CleanIO_mapper = []\n",
    "        for i in tqdm(range(0,len(self.IO_mapper))):\n",
    "            figure = cv2.imread(self.IO_mapper[i][0],0)\n",
    "            try:\n",
    "                shape = figure.shape\n",
    "            except:\n",
    "#                 print('Oh, you can not really drawing~')\n",
    "                continue\n",
    "            else:\n",
    "                if figure.shape != (28,28):\n",
    "                    continue\n",
    "                else:\n",
    "                    self.CleanIO_mapper.append(self.IO_mapper[i])\n",
    "#                     print('Oh, you can really drawing~')\n",
    "        self.IO_mapper = self.CleanIO_mapper\n",
    "        del self.CleanIO_mapper\n",
    "        print('Data loading finished,',len(self.IO_mapper),'remaining')\n",
    "        if anlysis:\n",
    "            self.analysis()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.IO_mapper)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = cv2.imread(self.IO_mapper[idx][0],0)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "        return image,self.IO_mapper[idx][1]\n",
    "    \n",
    "    def analysis(self):\n",
    "        print('Data analysising……')\n",
    "        self.mean = 0\n",
    "        self.std = 0\n",
    "        temp = []\n",
    "        for i in tqdm(self):\n",
    "            img = i[0].flatten() # 转为一维数组方便计算\n",
    "            self.mean += np.sum(img)\n",
    "        self.mean = self.mean / (len(self)*img.size)\n",
    "        print('Mean:',self.mean)\n",
    "        for i in self:\n",
    "            img = i[0].flatten() # 转为一维数组方便计算\n",
    "            self.std = self.std + np.sum((img - self.mean) ** 2)\n",
    "        self.std = np.sqrt(self.std / (len(self)*img.size))\n",
    "        print('Std:',self.std)\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33), interpolation=2), # resize+crop\n",
    "    transforms.RandomAffine((-15,15), translate=(0.1,0.3),shear=(-5,5), fillcolor=0),\n",
    "    transforms.RandomHorizontalFlip(p=0.3), # 30% 概率水平翻转\n",
    "    transforms.RandomRotation((-10,10)), # 随机旋转\n",
    "    torchvision.transforms.RandomPerspective(distortion_scale=0.3, p=0.3, interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.16443629118313927, ),(0.3239202530511322, )),\n",
    "])\n",
    "\n",
    "# Dataset & Dataloader\n",
    "# Draws = DrawDataset(['./train','./extra_training_data/data'],transform = train_transform)\n",
    "Draws = DrawDataset(['./train','./handwashed'],transform = train_transform)\n",
    "# Draws = DrawDataset(['./train'],transform = train_transform) # original\n",
    "dl = DataLoader(Draws, batch_size = 4, shuffle = True, num_workers = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "a = cv2.imread(Draws.IO_mapper[0][0],0).shape\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane ant bear flower \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img/2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "\n",
    "dataiter = iter(dl)\n",
    "images,labels = dataiter.next()\n",
    "\n",
    "k = list(Draws.label_mapper.keys()) # [1,2,3,5,4]\n",
    "v = list(Draws.label_mapper.values()) #[3,5,2,1,1]\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "labelshow = ''\n",
    "for i in labels:\n",
    "    labelshow += k[v.index(i)] + ' '\n",
    "print(labelshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will implement this model on: cuda !\n",
      "Image batch dimensions: torch.Size([512, 1, 224, 224])\n",
      "Image label dimensions: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 0.0001\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 200\n",
    "# NUM_EPOCHS = 200\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 224*224\n",
    "NUM_CLASSES = 15\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GRAYSCALE = True\n",
    "print('We will implement this model on:',DEVICE,'!')\n",
    "\n",
    "# for server-1\n",
    "# Datasets\n",
    "trainset = Draws\n",
    "\n",
    "# DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle = True, num_workers = 8)\n",
    "\n",
    "classes = ['airplane', 'ant', 'bear', 'bird', 'bridge',\n",
    "                        'bus', 'calendar', 'car', 'chair', 'dog',\n",
    "                        'dolphin', 'door', 'flower', 'fork', 'truck']\n",
    "\n",
    "# Checking datasets\n",
    "for images, labels in trainloader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(inp, oup, stride, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        conv_layer(inp, oup, 3, stride, 1, bias=False),\n",
    "        norm_layer(oup),\n",
    "        nlin_layer(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        conv_layer(inp, oup, 1, 1, 0, bias=False),\n",
    "        norm_layer(oup),\n",
    "        nlin_layer(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class Hswish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Hswish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\n",
    "class Hsigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Hsigmoid, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            Hsigmoid()\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class MobileBottleneck(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE'):\n",
    "        super(MobileBottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "        assert kernel in [3, 5]\n",
    "        padding = (kernel - 1) // 2\n",
    "        self.use_res_connect = stride == 1 and inp == oup\n",
    "\n",
    "        conv_layer = nn.Conv2d\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        if nl == 'RE':\n",
    "            nlin_layer = nn.ReLU # or ReLU6\n",
    "        elif nl == 'HS':\n",
    "            nlin_layer = Hswish\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if se:\n",
    "            SELayer = SEModule\n",
    "        else:\n",
    "            SELayer = Identity\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            conv_layer(inp, exp, 1, 1, 0, bias=False),\n",
    "            norm_layer(exp),\n",
    "            nlin_layer(inplace=True),\n",
    "            # dw\n",
    "            conv_layer(exp, exp, kernel, stride, padding, groups=exp, bias=False),\n",
    "            norm_layer(exp),\n",
    "            SELayer(exp),\n",
    "            nlin_layer(inplace=True),\n",
    "            # pw-linear\n",
    "            conv_layer(exp, oup, 1, 1, 0, bias=False),\n",
    "            norm_layer(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, dropout=0.8, mode='small', width_mult=1.0):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        input_channel = 16\n",
    "        last_channel = 1280\n",
    "        if mode == 'large':\n",
    "            # refer to Table 1 in paper\n",
    "            mobile_setting = [\n",
    "                # k, exp, c,  se,     nl,  s,\n",
    "                [3, 16,  16,  False, 'RE', 1],\n",
    "                [3, 64,  24,  False, 'RE', 2],\n",
    "                [3, 72,  24,  False, 'RE', 1],\n",
    "                [5, 72,  40,  True,  'RE', 2],\n",
    "                [5, 120, 40,  True,  'RE', 1],\n",
    "                [5, 120, 40,  True,  'RE', 1],\n",
    "                [3, 240, 80,  False, 'HS', 2],\n",
    "                [3, 200, 80,  False, 'HS', 1],\n",
    "                [3, 184, 80,  False, 'HS', 1],\n",
    "                [3, 184, 80,  False, 'HS', 1],\n",
    "                [3, 480, 112, True,  'HS', 1],\n",
    "                [3, 672, 112, True,  'HS', 1],\n",
    "                [5, 672, 160, True,  'HS', 2],\n",
    "                [5, 960, 160, True,  'HS', 1],\n",
    "                [5, 960, 160, True,  'HS', 1],\n",
    "            ]\n",
    "        elif mode == 'small':\n",
    "            # refer to Table 2 in paper\n",
    "            mobile_setting = [\n",
    "                # k, exp, c,  se,     nl,  s,\n",
    "                [3, 16,  16,  True,  'RE', 2],\n",
    "                [3, 72,  24,  False, 'RE', 2],\n",
    "                [3, 88,  24,  False, 'RE', 1],\n",
    "                [5, 96,  40,  True,  'HS', 2],\n",
    "                [5, 240, 40,  True,  'HS', 1],\n",
    "                [5, 240, 40,  True,  'HS', 1],\n",
    "                [5, 120, 48,  True,  'HS', 1],\n",
    "                [5, 144, 48,  True,  'HS', 1],\n",
    "                [5, 288, 96,  True,  'HS', 2],\n",
    "                [5, 576, 96,  True,  'HS', 1],\n",
    "                [5, 576, 96,  True,  'HS', 1],\n",
    "            ]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(1, input_channel, 2, nlin_layer=Hswish)]\n",
    "        self.classifier = []\n",
    "\n",
    "        # building mobile blocks\n",
    "        for k, exp, c, se, nl, s in mobile_setting:\n",
    "            output_channel = make_divisible(c * width_mult)\n",
    "            exp_channel = make_divisible(exp * width_mult)\n",
    "            self.features.append(MobileBottleneck(input_channel, output_channel, k, s, exp_channel, se, nl))\n",
    "            input_channel = output_channel\n",
    "\n",
    "        # building last several layers\n",
    "        if mode == 'large':\n",
    "            last_conv = make_divisible(960 * width_mult)\n",
    "            self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n",
    "            self.features.append(nn.AdaptiveAvgPool2d(1))\n",
    "            self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n",
    "            self.features.append(Hswish(inplace=True))\n",
    "        elif mode == 'small':\n",
    "            last_conv = make_divisible(576 * width_mult)\n",
    "            self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n",
    "            # self.features.append(SEModule(last_conv))  # refer to paper Table2, but I think this is a mistake\n",
    "            self.features.append(nn.AdaptiveAvgPool2d(1))\n",
    "            self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n",
    "            self.features.append(Hswish(inplace=True))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),    # refer to paper section 6\n",
    "            nn.Linear(last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def mobilenetv3(pretrained=False, **kwargs):\n",
    "    model = MobileNetV3(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load('mobilenetv3_small_67.4.pth.tar')\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        # raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mobilenetv3(n_class = NUM_CLASSES,input_size=224, dropout=0.8, mode='small', width_mult=1.0)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model.to(DEVICE)\n",
    "if(DEVICE == 'cuda'):\n",
    "    model=torch.nn.DataParallel(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/200 | Batch 0000/0020 | Cost: 2.7167\n",
      "Epoch: 001/200 | Train: 6.692%\n",
      "Time elapsed: 0.49 min\n",
      "Epoch: 002/200 | Batch 0000/0020 | Cost: 2.0016\n",
      "Epoch: 002/200 | Train: 11.150%\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 003/200 | Batch 0000/0020 | Cost: 1.7706\n",
      "Epoch: 003/200 | Train: 49.188%\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 004/200 | Batch 0000/0020 | Cost: 1.4702\n",
      "Epoch: 004/200 | Train: 43.382%\n",
      "Time elapsed: 1.42 min\n",
      "Epoch: 005/200 | Batch 0000/0020 | Cost: 1.2790\n",
      "Epoch: 005/200 | Train: 62.100%\n",
      "Time elapsed: 1.78 min\n",
      "Epoch: 006/200 | Batch 0000/0020 | Cost: 1.1426\n",
      "Epoch: 006/200 | Train: 58.055%\n",
      "Time elapsed: 2.12 min\n",
      "Epoch: 007/200 | Batch 0000/0020 | Cost: 1.0063\n",
      "Epoch: 007/200 | Train: 55.989%\n",
      "Time elapsed: 2.35 min\n",
      "Epoch: 008/200 | Batch 0000/0020 | Cost: 0.9500\n",
      "Epoch: 008/200 | Train: 66.519%\n",
      "Time elapsed: 2.62 min\n",
      "Epoch: 009/200 | Batch 0000/0020 | Cost: 0.9478\n",
      "Epoch: 009/200 | Train: 70.052%\n",
      "Time elapsed: 2.91 min\n",
      "Epoch: 010/200 | Batch 0000/0020 | Cost: 0.9153\n",
      "Epoch: 010/200 | Train: 69.088%\n",
      "Time elapsed: 3.26 min\n",
      "Epoch: 011/200 | Batch 0000/0020 | Cost: 0.8211\n",
      "Epoch: 011/200 | Train: 74.619%\n",
      "Time elapsed: 3.61 min\n",
      "Epoch: 012/200 | Batch 0000/0020 | Cost: 0.6684\n",
      "Epoch: 012/200 | Train: 74.343%\n",
      "Time elapsed: 3.97 min\n",
      "Epoch: 013/200 | Batch 0000/0020 | Cost: 0.7034\n",
      "Epoch: 013/200 | Train: 73.959%\n",
      "Time elapsed: 4.22 min\n",
      "Epoch: 014/200 | Batch 0000/0020 | Cost: 0.7301\n",
      "Epoch: 014/200 | Train: 76.666%\n",
      "Time elapsed: 4.48 min\n",
      "Epoch: 015/200 | Batch 0000/0020 | Cost: 0.6366\n",
      "Epoch: 015/200 | Train: 75.622%\n",
      "Time elapsed: 4.78 min\n",
      "Epoch: 016/200 | Batch 0000/0020 | Cost: 0.6796\n",
      "Epoch: 016/200 | Train: 73.733%\n",
      "Time elapsed: 5.14 min\n",
      "Epoch: 017/200 | Batch 0000/0020 | Cost: 0.6290\n",
      "Epoch: 017/200 | Train: 80.612%\n",
      "Time elapsed: 5.49 min\n",
      "Epoch: 018/200 | Batch 0000/0020 | Cost: 0.6048\n",
      "Epoch: 018/200 | Train: 80.120%\n",
      "Time elapsed: 5.83 min\n",
      "Epoch: 019/200 | Batch 0000/0020 | Cost: 0.5979\n",
      "Epoch: 019/200 | Train: 79.411%\n",
      "Time elapsed: 6.09 min\n",
      "Epoch: 020/200 | Batch 0000/0020 | Cost: 0.5182\n",
      "Epoch: 020/200 | Train: 79.894%\n",
      "Time elapsed: 6.33 min\n",
      "Epoch: 021/200 | Batch 0000/0020 | Cost: 0.5907\n",
      "Epoch: 021/200 | Train: 79.343%\n",
      "Time elapsed: 6.60 min\n",
      "Epoch: 022/200 | Batch 0000/0020 | Cost: 0.5187\n",
      "Epoch: 022/200 | Train: 80.307%\n",
      "Time elapsed: 6.95 min\n",
      "Epoch: 023/200 | Batch 0000/0020 | Cost: 0.5653\n",
      "Epoch: 023/200 | Train: 78.349%\n",
      "Time elapsed: 7.29 min\n",
      "Epoch: 024/200 | Batch 0000/0020 | Cost: 0.6026\n",
      "Epoch: 024/200 | Train: 78.053%\n",
      "Time elapsed: 7.66 min\n",
      "Epoch: 025/200 | Batch 0000/0020 | Cost: 0.5856\n",
      "Epoch: 025/200 | Train: 78.545%\n",
      "Time elapsed: 7.95 min\n",
      "Epoch: 026/200 | Batch 0000/0020 | Cost: 0.5152\n",
      "Epoch: 026/200 | Train: 82.866%\n",
      "Time elapsed: 8.19 min\n",
      "Epoch: 027/200 | Batch 0000/0020 | Cost: 0.5236\n",
      "Epoch: 027/200 | Train: 83.535%\n",
      "Time elapsed: 8.45 min\n",
      "Epoch: 028/200 | Batch 0000/0020 | Cost: 0.4550\n",
      "Epoch: 028/200 | Train: 80.976%\n",
      "Time elapsed: 8.80 min\n",
      "Epoch: 029/200 | Batch 0000/0020 | Cost: 0.4525\n",
      "Epoch: 029/200 | Train: 82.905%\n",
      "Time elapsed: 9.14 min\n",
      "Epoch: 030/200 | Batch 0000/0020 | Cost: 0.4738\n",
      "Epoch: 030/200 | Train: 82.925%\n",
      "Time elapsed: 9.50 min\n",
      "Epoch: 031/200 | Batch 0000/0020 | Cost: 0.3968\n",
      "Epoch: 031/200 | Train: 83.860%\n",
      "Time elapsed: 9.81 min\n",
      "Epoch: 032/200 | Batch 0000/0020 | Cost: 0.5034\n",
      "Epoch: 032/200 | Train: 82.758%\n",
      "Time elapsed: 10.00 min\n",
      "Epoch: 033/200 | Batch 0000/0020 | Cost: 0.4640\n",
      "Epoch: 033/200 | Train: 84.746%\n",
      "Time elapsed: 10.26 min\n",
      "Epoch: 034/200 | Batch 0000/0020 | Cost: 0.4916\n",
      "Epoch: 034/200 | Train: 85.208%\n",
      "Time elapsed: 10.58 min\n",
      "Epoch: 035/200 | Batch 0000/0020 | Cost: 0.4367\n",
      "Epoch: 035/200 | Train: 83.939%\n",
      "Time elapsed: 10.93 min\n",
      "Epoch: 036/200 | Batch 0000/0020 | Cost: 0.4407\n",
      "Epoch: 036/200 | Train: 83.260%\n",
      "Time elapsed: 11.28 min\n",
      "Epoch: 037/200 | Batch 0000/0020 | Cost: 0.3670\n",
      "Epoch: 037/200 | Train: 85.523%\n",
      "Time elapsed: 11.62 min\n",
      "Epoch: 038/200 | Batch 0000/0020 | Cost: 0.4152\n",
      "Epoch: 038/200 | Train: 85.543%\n",
      "Time elapsed: 11.83 min\n",
      "Epoch: 039/200 | Batch 0000/0020 | Cost: 0.3556\n",
      "Epoch: 039/200 | Train: 83.998%\n",
      "Time elapsed: 12.09 min\n",
      "Epoch: 040/200 | Batch 0000/0020 | Cost: 0.4087\n",
      "Epoch: 040/200 | Train: 84.598%\n",
      "Time elapsed: 12.38 min\n",
      "Epoch: 041/200 | Batch 0000/0020 | Cost: 0.3922\n",
      "Epoch: 041/200 | Train: 86.842%\n",
      "Time elapsed: 12.73 min\n",
      "Epoch: 042/200 | Batch 0000/0020 | Cost: 0.4179\n",
      "Epoch: 042/200 | Train: 85.681%\n",
      "Time elapsed: 13.09 min\n",
      "Epoch: 043/200 | Batch 0000/0020 | Cost: 0.4101\n",
      "Epoch: 043/200 | Train: 85.415%\n",
      "Time elapsed: 13.44 min\n",
      "Epoch: 044/200 | Batch 0000/0020 | Cost: 0.3788\n",
      "Epoch: 044/200 | Train: 86.930%\n",
      "Time elapsed: 13.67 min\n",
      "Epoch: 045/200 | Batch 0000/0020 | Cost: 0.3517\n",
      "Epoch: 045/200 | Train: 87.964%\n",
      "Time elapsed: 13.93 min\n",
      "Epoch: 046/200 | Batch 0000/0020 | Cost: 0.3380\n",
      "Epoch: 046/200 | Train: 88.121%\n",
      "Time elapsed: 14.24 min\n",
      "Epoch: 047/200 | Batch 0000/0020 | Cost: 0.3148\n",
      "Epoch: 047/200 | Train: 86.665%\n",
      "Time elapsed: 14.61 min\n",
      "Epoch: 048/200 | Batch 0000/0020 | Cost: 0.4282\n",
      "Epoch: 048/200 | Train: 86.497%\n",
      "Time elapsed: 14.97 min\n",
      "Epoch: 049/200 | Batch 0000/0020 | Cost: 0.3708\n",
      "Epoch: 049/200 | Train: 83.712%\n",
      "Time elapsed: 15.31 min\n",
      "Epoch: 050/200 | Batch 0000/0020 | Cost: 0.4569\n",
      "Epoch: 050/200 | Train: 86.783%\n",
      "Time elapsed: 15.52 min\n",
      "Epoch: 051/200 | Batch 0000/0020 | Cost: 0.3737\n",
      "Epoch: 051/200 | Train: 87.619%\n",
      "Time elapsed: 15.79 min\n",
      "Epoch: 052/200 | Batch 0000/0020 | Cost: 0.3205\n",
      "Epoch: 052/200 | Train: 88.082%\n",
      "Time elapsed: 16.07 min\n",
      "Epoch: 053/200 | Batch 0000/0020 | Cost: 0.3738\n",
      "Epoch: 053/200 | Train: 87.600%\n",
      "Time elapsed: 16.43 min\n",
      "Epoch: 054/200 | Batch 0000/0020 | Cost: 0.3386\n",
      "Epoch: 054/200 | Train: 87.590%\n",
      "Time elapsed: 16.79 min\n",
      "Epoch: 055/200 | Batch 0000/0020 | Cost: 0.2966\n",
      "Epoch: 055/200 | Train: 86.802%\n",
      "Time elapsed: 17.15 min\n",
      "Epoch: 056/200 | Batch 0000/0020 | Cost: 0.3130\n",
      "Epoch: 056/200 | Train: 86.783%\n",
      "Time elapsed: 17.40 min\n",
      "Epoch: 057/200 | Batch 0000/0020 | Cost: 0.3857\n",
      "Epoch: 057/200 | Train: 85.218%\n",
      "Epoch    56: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Time elapsed: 17.67 min\n",
      "Epoch: 058/200 | Batch 0000/0020 | Cost: 0.3687\n",
      "Epoch: 058/200 | Train: 89.863%\n",
      "Time elapsed: 17.96 min\n",
      "Epoch: 059/200 | Batch 0000/0020 | Cost: 0.3068\n",
      "Epoch: 059/200 | Train: 92.087%\n",
      "Time elapsed: 18.33 min\n",
      "Epoch: 060/200 | Batch 0000/0020 | Cost: 0.2503\n",
      "Epoch: 060/200 | Train: 91.881%\n",
      "Time elapsed: 18.70 min\n",
      "Epoch: 061/200 | Batch 0000/0020 | Cost: 0.2099\n",
      "Epoch: 061/200 | Train: 92.215%\n",
      "Time elapsed: 19.04 min\n",
      "Epoch: 062/200 | Batch 0000/0020 | Cost: 0.1831\n",
      "Epoch: 062/200 | Train: 91.576%\n",
      "Time elapsed: 19.29 min\n",
      "Epoch: 063/200 | Batch 0000/0020 | Cost: 0.1902\n",
      "Epoch: 063/200 | Train: 91.989%\n",
      "Time elapsed: 19.55 min\n",
      "Epoch: 064/200 | Batch 0000/0020 | Cost: 0.2173\n",
      "Epoch: 064/200 | Train: 92.333%\n",
      "Time elapsed: 19.86 min\n",
      "Epoch: 065/200 | Batch 0000/0020 | Cost: 0.2546\n",
      "Epoch: 065/200 | Train: 92.471%\n",
      "Time elapsed: 20.20 min\n",
      "Epoch: 066/200 | Batch 0000/0020 | Cost: 0.2837\n",
      "Epoch: 066/200 | Train: 92.560%\n",
      "Time elapsed: 20.56 min\n",
      "Epoch: 067/200 | Batch 0000/0020 | Cost: 0.2332\n",
      "Epoch: 067/200 | Train: 92.707%\n",
      "Time elapsed: 20.93 min\n",
      "Epoch: 068/200 | Batch 0000/0020 | Cost: 0.2705\n",
      "Epoch: 068/200 | Train: 93.219%\n",
      "Time elapsed: 21.19 min\n",
      "Epoch: 069/200 | Batch 0000/0020 | Cost: 0.2571\n",
      "Epoch: 069/200 | Train: 92.707%\n",
      "Time elapsed: 21.46 min\n",
      "Epoch: 070/200 | Batch 0000/0020 | Cost: 0.2037\n",
      "Epoch: 070/200 | Train: 92.560%\n",
      "Time elapsed: 21.75 min\n",
      "Epoch: 071/200 | Batch 0000/0020 | Cost: 0.2284\n",
      "Epoch: 071/200 | Train: 92.530%\n",
      "Time elapsed: 22.10 min\n",
      "Epoch: 072/200 | Batch 0000/0020 | Cost: 0.1526\n",
      "Epoch: 072/200 | Train: 93.514%\n",
      "Time elapsed: 22.44 min\n",
      "Epoch: 073/200 | Batch 0000/0020 | Cost: 0.2334\n",
      "Epoch: 073/200 | Train: 92.412%\n",
      "Time elapsed: 22.79 min\n",
      "Epoch: 074/200 | Batch 0000/0020 | Cost: 0.2431\n",
      "Epoch: 074/200 | Train: 92.304%\n",
      "Time elapsed: 23.06 min\n",
      "Epoch: 075/200 | Batch 0000/0020 | Cost: 0.2012\n",
      "Epoch: 075/200 | Train: 93.150%\n",
      "Time elapsed: 23.31 min\n",
      "Epoch: 076/200 | Batch 0000/0020 | Cost: 0.1793\n",
      "Epoch: 076/200 | Train: 92.471%\n",
      "Time elapsed: 23.60 min\n",
      "Epoch: 077/200 | Batch 0000/0020 | Cost: 0.1832\n",
      "Epoch: 077/200 | Train: 93.534%\n",
      "Time elapsed: 23.97 min\n",
      "Epoch: 078/200 | Batch 0000/0020 | Cost: 0.2138\n",
      "Epoch: 078/200 | Train: 93.288%\n",
      "Time elapsed: 24.33 min\n",
      "Epoch: 079/200 | Batch 0000/0020 | Cost: 0.2209\n",
      "Epoch: 079/200 | Train: 93.072%\n",
      "Time elapsed: 24.68 min\n",
      "Epoch: 080/200 | Batch 0000/0020 | Cost: 0.1678\n",
      "Epoch: 080/200 | Train: 91.674%\n",
      "Time elapsed: 24.95 min\n",
      "Epoch: 081/200 | Batch 0000/0020 | Cost: 0.2189\n",
      "Epoch: 081/200 | Train: 92.688%\n",
      "Time elapsed: 25.18 min\n",
      "Epoch: 082/200 | Batch 0000/0020 | Cost: 0.1963\n",
      "Epoch: 082/200 | Train: 92.629%\n",
      "Time elapsed: 25.44 min\n",
      "Epoch: 083/200 | Batch 0000/0020 | Cost: 0.2172\n",
      "Epoch: 083/200 | Train: 93.081%\n",
      "Time elapsed: 25.79 min\n",
      "Epoch: 084/200 | Batch 0000/0020 | Cost: 0.2607\n",
      "Epoch: 084/200 | Train: 93.505%\n",
      "Time elapsed: 26.15 min\n",
      "Epoch: 085/200 | Batch 0000/0020 | Cost: 0.1852\n",
      "Epoch: 085/200 | Train: 92.255%\n",
      "Time elapsed: 26.50 min\n",
      "Epoch: 086/200 | Batch 0000/0020 | Cost: 0.2333\n",
      "Epoch: 086/200 | Train: 92.678%\n",
      "Time elapsed: 26.81 min\n",
      "Epoch: 087/200 | Batch 0000/0020 | Cost: 0.2260\n",
      "Epoch: 087/200 | Train: 92.806%\n",
      "Time elapsed: 27.00 min\n",
      "Epoch: 088/200 | Batch 0000/0020 | Cost: 0.2105\n",
      "Epoch: 088/200 | Train: 92.019%\n",
      "Epoch    87: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Time elapsed: 27.25 min\n",
      "Epoch: 089/200 | Batch 0000/0020 | Cost: 0.1685\n",
      "Epoch: 089/200 | Train: 95.119%\n",
      "Time elapsed: 27.58 min\n",
      "Epoch: 090/200 | Batch 0000/0020 | Cost: 0.1460\n",
      "Epoch: 090/200 | Train: 95.276%\n",
      "Time elapsed: 27.95 min\n",
      "Epoch: 091/200 | Batch 0000/0020 | Cost: 0.2377\n",
      "Epoch: 091/200 | Train: 95.630%\n",
      "Time elapsed: 28.31 min\n",
      "Epoch: 092/200 | Batch 0000/0020 | Cost: 0.1344\n",
      "Epoch: 092/200 | Train: 96.014%\n",
      "Time elapsed: 28.63 min\n",
      "Epoch: 093/200 | Batch 0000/0020 | Cost: 0.1550\n",
      "Epoch: 093/200 | Train: 95.670%\n",
      "Time elapsed: 28.83 min\n",
      "Epoch: 094/200 | Batch 0000/0020 | Cost: 0.1646\n",
      "Epoch: 094/200 | Train: 95.729%\n",
      "Time elapsed: 29.09 min\n",
      "Epoch: 095/200 | Batch 0000/0020 | Cost: 0.1183\n",
      "Epoch: 095/200 | Train: 95.561%\n",
      "Time elapsed: 29.38 min\n",
      "Epoch: 096/200 | Batch 0000/0020 | Cost: 0.1500\n",
      "Epoch: 096/200 | Train: 96.378%\n",
      "Time elapsed: 29.73 min\n",
      "Epoch: 097/200 | Batch 0000/0020 | Cost: 0.1336\n",
      "Epoch: 097/200 | Train: 95.916%\n",
      "Time elapsed: 30.09 min\n",
      "Epoch: 098/200 | Batch 0000/0020 | Cost: 0.1429\n",
      "Epoch: 098/200 | Train: 95.670%\n",
      "Time elapsed: 30.45 min\n",
      "Epoch: 099/200 | Batch 0000/0020 | Cost: 0.1703\n",
      "Epoch: 099/200 | Train: 95.994%\n",
      "Time elapsed: 30.68 min\n",
      "Epoch: 100/200 | Batch 0000/0020 | Cost: 0.1101\n",
      "Epoch: 100/200 | Train: 95.847%\n",
      "Time elapsed: 30.94 min\n",
      "Epoch: 101/200 | Batch 0000/0020 | Cost: 0.1493\n",
      "Epoch: 101/200 | Train: 95.847%\n",
      "Time elapsed: 31.22 min\n",
      "Epoch: 102/200 | Batch 0000/0020 | Cost: 0.1545\n",
      "Epoch: 102/200 | Train: 96.024%\n",
      "Time elapsed: 31.58 min\n",
      "Epoch: 103/200 | Batch 0000/0020 | Cost: 0.1381\n",
      "Epoch: 103/200 | Train: 96.172%\n",
      "Time elapsed: 31.93 min\n",
      "Epoch: 104/200 | Batch 0000/0020 | Cost: 0.1484\n",
      "Epoch: 104/200 | Train: 96.132%\n",
      "Time elapsed: 32.24 min\n",
      "Epoch: 105/200 | Batch 0000/0020 | Cost: 0.1230\n",
      "Epoch: 105/200 | Train: 95.640%\n",
      "Time elapsed: 32.47 min\n",
      "Epoch: 106/200 | Batch 0000/0020 | Cost: 0.1142\n",
      "Epoch: 106/200 | Train: 95.611%\n",
      "Time elapsed: 32.69 min\n",
      "Epoch: 107/200 | Batch 0000/0020 | Cost: 0.1142\n",
      "Epoch: 107/200 | Train: 95.650%\n",
      "Epoch   106: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Time elapsed: 32.93 min\n",
      "Epoch: 108/200 | Batch 0000/0020 | Cost: 0.1143\n",
      "Epoch: 108/200 | Train: 96.713%\n",
      "Time elapsed: 33.28 min\n",
      "Epoch: 109/200 | Batch 0000/0020 | Cost: 0.1246\n",
      "Epoch: 109/200 | Train: 97.038%\n",
      "Time elapsed: 33.63 min\n",
      "Epoch: 110/200 | Batch 0000/0020 | Cost: 0.1100\n",
      "Epoch: 110/200 | Train: 96.988%\n",
      "Time elapsed: 34.01 min\n",
      "Epoch: 111/200 | Batch 0000/0020 | Cost: 0.1056\n",
      "Epoch: 111/200 | Train: 97.195%\n",
      "Time elapsed: 34.31 min\n",
      "Epoch: 112/200 | Batch 0000/0020 | Cost: 0.1033\n",
      "Epoch: 112/200 | Train: 97.284%\n",
      "Time elapsed: 34.53 min\n",
      "Epoch: 113/200 | Batch 0000/0020 | Cost: 0.1059\n",
      "Epoch: 113/200 | Train: 97.166%\n",
      "Time elapsed: 34.78 min\n",
      "Epoch: 114/200 | Batch 0000/0020 | Cost: 0.1124\n",
      "Epoch: 114/200 | Train: 97.195%\n",
      "Time elapsed: 35.12 min\n",
      "Epoch: 115/200 | Batch 0000/0020 | Cost: 0.1192\n",
      "Epoch: 115/200 | Train: 97.303%\n",
      "Time elapsed: 35.48 min\n",
      "Epoch: 116/200 | Batch 0000/0020 | Cost: 0.1037\n",
      "Epoch: 116/200 | Train: 97.087%\n",
      "Time elapsed: 35.83 min\n",
      "Epoch: 117/200 | Batch 0000/0020 | Cost: 0.1048\n",
      "Epoch: 117/200 | Train: 96.959%\n",
      "Time elapsed: 36.14 min\n",
      "Epoch: 118/200 | Batch 0000/0020 | Cost: 0.0898\n",
      "Epoch: 118/200 | Train: 97.461%\n",
      "Time elapsed: 36.34 min\n",
      "Epoch: 119/200 | Batch 0000/0020 | Cost: 0.0851\n",
      "Epoch: 119/200 | Train: 97.235%\n",
      "Time elapsed: 36.60 min\n",
      "Epoch: 120/200 | Batch 0000/0020 | Cost: 0.1065\n",
      "Epoch: 120/200 | Train: 97.323%\n",
      "Time elapsed: 36.90 min\n",
      "Epoch: 121/200 | Batch 0000/0020 | Cost: 0.1269\n",
      "Epoch: 121/200 | Train: 97.225%\n",
      "Time elapsed: 37.25 min\n",
      "Epoch: 122/200 | Batch 0000/0020 | Cost: 0.0750\n",
      "Epoch: 122/200 | Train: 96.654%\n",
      "Time elapsed: 37.60 min\n",
      "Epoch: 123/200 | Batch 0000/0020 | Cost: 0.1012\n",
      "Epoch: 123/200 | Train: 97.353%\n",
      "Time elapsed: 37.95 min\n",
      "Epoch: 124/200 | Batch 0000/0020 | Cost: 0.1037\n",
      "Epoch: 124/200 | Train: 97.323%\n",
      "Time elapsed: 38.16 min\n",
      "Epoch: 125/200 | Batch 0000/0020 | Cost: 0.1127\n",
      "Epoch: 125/200 | Train: 97.589%\n",
      "Time elapsed: 38.42 min\n",
      "Epoch: 126/200 | Batch 0000/0020 | Cost: 0.0773\n",
      "Epoch: 126/200 | Train: 97.156%\n",
      "Time elapsed: 38.71 min\n",
      "Epoch: 127/200 | Batch 0000/0020 | Cost: 0.0849\n",
      "Epoch: 127/200 | Train: 97.353%\n",
      "Time elapsed: 39.08 min\n",
      "Epoch: 128/200 | Batch 0000/0020 | Cost: 0.1022\n",
      "Epoch: 128/200 | Train: 97.451%\n",
      "Time elapsed: 39.42 min\n",
      "Epoch: 129/200 | Batch 0000/0020 | Cost: 0.1064\n",
      "Epoch: 129/200 | Train: 97.067%\n",
      "Time elapsed: 39.80 min\n",
      "Epoch: 130/200 | Batch 0000/0020 | Cost: 0.0707\n",
      "Epoch: 130/200 | Train: 97.579%\n",
      "Time elapsed: 40.02 min\n",
      "Epoch: 131/200 | Batch 0000/0020 | Cost: 0.0610\n",
      "Epoch: 131/200 | Train: 97.668%\n",
      "Time elapsed: 40.28 min\n",
      "Epoch: 132/200 | Batch 0000/0020 | Cost: 0.0828\n",
      "Epoch: 132/200 | Train: 97.412%\n",
      "Time elapsed: 40.56 min\n",
      "Epoch: 133/200 | Batch 0000/0020 | Cost: 0.1017\n",
      "Epoch: 133/200 | Train: 97.530%\n",
      "Time elapsed: 40.91 min\n",
      "Epoch: 134/200 | Batch 0000/0020 | Cost: 0.1061\n",
      "Epoch: 134/200 | Train: 97.382%\n",
      "Time elapsed: 41.26 min\n",
      "Epoch: 135/200 | Batch 0000/0020 | Cost: 0.0768\n",
      "Epoch: 135/200 | Train: 97.412%\n",
      "Time elapsed: 41.61 min\n",
      "Epoch: 136/200 | Batch 0000/0020 | Cost: 0.0921\n",
      "Epoch: 136/200 | Train: 97.520%\n",
      "Time elapsed: 41.84 min\n",
      "Epoch: 137/200 | Batch 0000/0020 | Cost: 0.0837\n",
      "Epoch: 137/200 | Train: 97.392%\n",
      "Time elapsed: 42.10 min\n",
      "Epoch: 138/200 | Batch 0000/0020 | Cost: 0.1274\n",
      "Epoch: 138/200 | Train: 97.490%\n",
      "Time elapsed: 42.40 min\n",
      "Epoch: 139/200 | Batch 0000/0020 | Cost: 0.1278\n",
      "Epoch: 139/200 | Train: 97.687%\n",
      "Time elapsed: 42.71 min\n",
      "Epoch: 140/200 | Batch 0000/0020 | Cost: 0.0770\n",
      "Epoch: 140/200 | Train: 97.530%\n",
      "Time elapsed: 42.98 min\n",
      "Epoch: 141/200 | Batch 0000/0020 | Cost: 0.0702\n",
      "Epoch: 141/200 | Train: 97.530%\n",
      "Time elapsed: 43.31 min\n",
      "Epoch: 142/200 | Batch 0000/0020 | Cost: 0.0890\n",
      "Epoch: 142/200 | Train: 97.353%\n",
      "Time elapsed: 43.66 min\n",
      "Epoch: 143/200 | Batch 0000/0020 | Cost: 0.0838\n",
      "Epoch: 143/200 | Train: 97.628%\n",
      "Time elapsed: 44.02 min\n",
      "Epoch: 144/200 | Batch 0000/0020 | Cost: 0.1093\n",
      "Epoch: 144/200 | Train: 97.569%\n",
      "Time elapsed: 44.27 min\n",
      "Epoch: 145/200 | Batch 0000/0020 | Cost: 0.0958\n",
      "Epoch: 145/200 | Train: 97.638%\n",
      "Time elapsed: 44.53 min\n",
      "Epoch: 146/200 | Batch 0000/0020 | Cost: 0.0913\n",
      "Epoch: 146/200 | Train: 97.727%\n",
      "Time elapsed: 44.83 min\n",
      "Epoch: 147/200 | Batch 0000/0020 | Cost: 0.1454\n",
      "Epoch: 147/200 | Train: 97.431%\n",
      "Time elapsed: 45.19 min\n",
      "Epoch: 148/200 | Batch 0000/0020 | Cost: 0.0870\n",
      "Epoch: 148/200 | Train: 97.884%\n",
      "Time elapsed: 45.53 min\n",
      "Epoch: 149/200 | Batch 0000/0020 | Cost: 0.0823\n",
      "Epoch: 149/200 | Train: 97.166%\n",
      "Time elapsed: 45.87 min\n",
      "Epoch: 150/200 | Batch 0000/0020 | Cost: 0.0937\n",
      "Epoch: 150/200 | Train: 97.362%\n",
      "Time elapsed: 46.07 min\n",
      "Epoch: 151/200 | Batch 0000/0020 | Cost: 0.0762\n",
      "Epoch: 151/200 | Train: 97.264%\n",
      "Time elapsed: 46.33 min\n",
      "Epoch: 152/200 | Batch 0000/0020 | Cost: 0.1034\n",
      "Epoch: 152/200 | Train: 97.992%\n",
      "Time elapsed: 46.62 min\n",
      "Epoch: 153/200 | Batch 0000/0020 | Cost: 0.0672\n",
      "Epoch: 153/200 | Train: 97.864%\n",
      "Time elapsed: 46.99 min\n",
      "Epoch: 154/200 | Batch 0000/0020 | Cost: 0.0648\n",
      "Epoch: 154/200 | Train: 97.884%\n",
      "Time elapsed: 47.15 min\n",
      "Epoch: 155/200 | Batch 0000/0020 | Cost: 0.0715\n",
      "Epoch: 155/200 | Train: 97.717%\n",
      "Time elapsed: 47.31 min\n",
      "Epoch: 156/200 | Batch 0000/0020 | Cost: 0.0684\n",
      "Epoch: 156/200 | Train: 97.746%\n",
      "Time elapsed: 47.48 min\n",
      "Epoch: 157/200 | Batch 0000/0020 | Cost: 0.1006\n",
      "Epoch: 157/200 | Train: 97.658%\n",
      "Time elapsed: 47.64 min\n",
      "Epoch: 158/200 | Batch 0000/0020 | Cost: 0.1075\n",
      "Epoch: 158/200 | Train: 97.835%\n",
      "Time elapsed: 47.84 min\n",
      "Epoch: 159/200 | Batch 0000/0020 | Cost: 0.1152\n",
      "Epoch: 159/200 | Train: 97.992%\n",
      "Time elapsed: 48.18 min\n",
      "Epoch: 160/200 | Batch 0000/0020 | Cost: 0.0673\n",
      "Epoch: 160/200 | Train: 97.589%\n",
      "Time elapsed: 48.52 min\n",
      "Epoch: 161/200 | Batch 0000/0020 | Cost: 0.0957\n",
      "Epoch: 161/200 | Train: 97.589%\n",
      "Time elapsed: 48.87 min\n",
      "Epoch: 162/200 | Batch 0000/0020 | Cost: 0.0799\n",
      "Epoch: 162/200 | Train: 97.884%\n",
      "Time elapsed: 49.16 min\n",
      "Epoch: 163/200 | Batch 0000/0020 | Cost: 0.0726\n",
      "Epoch: 163/200 | Train: 97.677%\n",
      "Epoch   162: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Time elapsed: 49.35 min\n",
      "Epoch: 164/200 | Batch 0000/0020 | Cost: 0.0943\n",
      "Epoch: 164/200 | Train: 98.081%\n",
      "Time elapsed: 49.62 min\n",
      "Epoch: 165/200 | Batch 0000/0020 | Cost: 0.0710\n",
      "Epoch: 165/200 | Train: 97.845%\n",
      "Time elapsed: 49.92 min\n",
      "Epoch: 166/200 | Batch 0000/0020 | Cost: 0.0626\n",
      "Epoch: 166/200 | Train: 98.071%\n",
      "Time elapsed: 50.28 min\n",
      "Epoch: 167/200 | Batch 0000/0020 | Cost: 0.0690\n",
      "Epoch: 167/200 | Train: 98.150%\n",
      "Time elapsed: 50.63 min\n",
      "Epoch: 168/200 | Batch 0000/0020 | Cost: 0.0819\n",
      "Epoch: 168/200 | Train: 98.150%\n",
      "Time elapsed: 50.97 min\n",
      "Epoch: 169/200 | Batch 0000/0020 | Cost: 0.0736\n",
      "Epoch: 169/200 | Train: 98.248%\n",
      "Time elapsed: 51.19 min\n",
      "Epoch: 170/200 | Batch 0000/0020 | Cost: 0.0855\n",
      "Epoch: 170/200 | Train: 98.061%\n",
      "Time elapsed: 51.45 min\n",
      "Epoch: 171/200 | Batch 0000/0020 | Cost: 0.0729\n",
      "Epoch: 171/200 | Train: 98.061%\n",
      "Time elapsed: 51.73 min\n",
      "Epoch: 172/200 | Batch 0000/0020 | Cost: 0.0945\n",
      "Epoch: 172/200 | Train: 98.032%\n",
      "Time elapsed: 52.09 min\n",
      "Epoch: 173/200 | Batch 0000/0020 | Cost: 0.0653\n",
      "Epoch: 173/200 | Train: 98.229%\n",
      "Time elapsed: 52.42 min\n",
      "Epoch: 174/200 | Batch 0000/0020 | Cost: 0.0670\n",
      "Epoch: 174/200 | Train: 98.022%\n",
      "Time elapsed: 52.77 min\n",
      "Epoch: 175/200 | Batch 0000/0020 | Cost: 0.0506\n",
      "Epoch: 175/200 | Train: 98.297%\n",
      "Time elapsed: 53.01 min\n",
      "Epoch: 176/200 | Batch 0000/0020 | Cost: 0.0636\n",
      "Epoch: 176/200 | Train: 98.347%\n",
      "Time elapsed: 53.28 min\n",
      "Epoch: 177/200 | Batch 0000/0020 | Cost: 0.0820\n",
      "Epoch: 177/200 | Train: 98.307%\n",
      "Time elapsed: 53.58 min\n",
      "Epoch: 178/200 | Batch 0000/0020 | Cost: 0.0807\n",
      "Epoch: 178/200 | Train: 98.337%\n",
      "Time elapsed: 53.93 min\n",
      "Epoch: 179/200 | Batch 0000/0020 | Cost: 0.0974\n",
      "Epoch: 179/200 | Train: 97.904%\n",
      "Time elapsed: 54.28 min\n",
      "Epoch: 180/200 | Batch 0000/0020 | Cost: 0.0441\n",
      "Epoch: 180/200 | Train: 98.396%\n",
      "Time elapsed: 54.63 min\n",
      "Epoch: 181/200 | Batch 0000/0020 | Cost: 0.1096\n",
      "Epoch: 181/200 | Train: 98.445%\n",
      "Time elapsed: 54.87 min\n",
      "Epoch: 182/200 | Batch 0000/0020 | Cost: 0.0648\n",
      "Epoch: 182/200 | Train: 98.209%\n",
      "Time elapsed: 55.13 min\n",
      "Epoch: 183/200 | Batch 0000/0020 | Cost: 0.0904\n",
      "Epoch: 183/200 | Train: 98.297%\n",
      "Time elapsed: 55.43 min\n",
      "Epoch: 184/200 | Batch 0000/0020 | Cost: 0.0690\n",
      "Epoch: 184/200 | Train: 98.179%\n",
      "Time elapsed: 55.78 min\n",
      "Epoch: 185/200 | Batch 0000/0020 | Cost: 0.0438\n",
      "Epoch: 185/200 | Train: 98.356%\n",
      "Time elapsed: 56.13 min\n",
      "Epoch: 186/200 | Batch 0000/0020 | Cost: 0.0861\n",
      "Epoch: 186/200 | Train: 98.258%\n",
      "Time elapsed: 56.49 min\n",
      "Epoch: 187/200 | Batch 0000/0020 | Cost: 0.0739\n",
      "Epoch: 187/200 | Train: 98.376%\n",
      "Time elapsed: 56.72 min\n",
      "Epoch: 188/200 | Batch 0000/0020 | Cost: 0.0691\n",
      "Epoch: 188/200 | Train: 98.406%\n",
      "Time elapsed: 56.98 min\n",
      "Epoch: 189/200 | Batch 0000/0020 | Cost: 0.0582\n",
      "Epoch: 189/200 | Train: 98.297%\n",
      "Time elapsed: 57.27 min\n",
      "Epoch: 190/200 | Batch 0000/0020 | Cost: 0.0640\n",
      "Epoch: 190/200 | Train: 98.199%\n",
      "Time elapsed: 57.63 min\n",
      "Epoch: 191/200 | Batch 0000/0020 | Cost: 0.0621\n",
      "Epoch: 191/200 | Train: 98.150%\n",
      "Time elapsed: 58.00 min\n",
      "Epoch: 192/200 | Batch 0000/0020 | Cost: 0.0956\n",
      "Epoch: 192/200 | Train: 98.425%\n",
      "Epoch   191: reducing learning rate of group 0 to 1.5625e-04.\n",
      "Time elapsed: 58.32 min\n",
      "Epoch: 193/200 | Batch 0000/0020 | Cost: 0.1114\n",
      "Epoch: 193/200 | Train: 98.268%\n",
      "Time elapsed: 58.55 min\n",
      "Epoch: 194/200 | Batch 0000/0020 | Cost: 0.0776\n",
      "Epoch: 194/200 | Train: 98.534%\n",
      "Time elapsed: 58.83 min\n",
      "Epoch: 195/200 | Batch 0000/0020 | Cost: 0.0471\n",
      "Epoch: 195/200 | Train: 98.435%\n",
      "Time elapsed: 59.12 min\n",
      "Epoch: 196/200 | Batch 0000/0020 | Cost: 0.0680\n",
      "Epoch: 196/200 | Train: 98.406%\n",
      "Time elapsed: 59.47 min\n",
      "Epoch: 197/200 | Batch 0000/0020 | Cost: 0.0441\n",
      "Epoch: 197/200 | Train: 98.297%\n",
      "Time elapsed: 59.82 min\n",
      "Epoch: 198/200 | Batch 0000/0020 | Cost: 0.0508\n",
      "Epoch: 198/200 | Train: 98.445%\n",
      "Time elapsed: 60.16 min\n",
      "Epoch: 199/200 | Batch 0000/0020 | Cost: 0.1047\n",
      "Epoch: 199/200 | Train: 98.209%\n",
      "Time elapsed: 60.41 min\n",
      "Epoch: 200/200 | Batch 0000/0020 | Cost: 0.0562\n",
      "Epoch: 200/200 | Train: 98.475%\n",
      "Time elapsed: 60.66 min\n",
      "Total Training Time: 60.66 min\n",
      "Test accuracy: 98.57%\n",
      "Total Time: 60.77 min\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=5e-06)\n",
    "\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(trainloader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "#         scheduler.zero_grad()\n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 70:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(trainloader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        now_acc = compute_accuracy(model, trainloader, device=DEVICE)\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              now_acc))\n",
    "        scheduler.step(now_acc)\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, trainloader, device=DEVICE)))\n",
    "    \n",
    "print('Total Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "\n",
    "def save_model(the_model, PATH = 'final.pkl'):\n",
    "    torch.save(the_model.module.state_dict(), PATH)\n",
    "\n",
    "\n",
    "    \n",
    "save_model(model,'mov3.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(the_model, PATH = 'mov3.pkl'):\n",
    "    torch.save(the_model.module.state_dict(), PATH)\n",
    "\n",
    "\n",
    "    \n",
    "save_model(model,'mov3.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MobileNetV3(\n",
       "    (features): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hswish()\n",
       "      )\n",
       "      (1): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=16, out_features=4, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=4, out_features=16, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): ReLU(inplace)\n",
       "          (7): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Identity()\n",
       "          (6): ReLU(inplace)\n",
       "          (7): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): Identity()\n",
       "          (6): ReLU(inplace)\n",
       "          (7): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=96, out_features=24, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=24, out_features=96, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=240, out_features=60, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=60, out_features=240, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=240, out_features=60, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=60, out_features=240, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=120, out_features=30, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=30, out_features=120, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=144, out_features=36, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=36, out_features=144, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=288, out_features=72, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=72, out_features=288, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=576, out_features=144, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=144, out_features=576, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): MobileBottleneck(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): Hswish()\n",
       "          (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): SEModule(\n",
       "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc): Sequential(\n",
       "              (0): Linear(in_features=576, out_features=144, bias=False)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Linear(in_features=144, out_features=576, bias=False)\n",
       "              (3): Hsigmoid()\n",
       "            )\n",
       "          )\n",
       "          (6): Hswish()\n",
       "          (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): Sequential(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Hswish()\n",
       "      )\n",
       "      (13): AdaptiveAvgPool2d(output_size=1)\n",
       "      (14): Conv2d(576, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (15): Hswish()\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.8)\n",
       "      (1): Linear(in_features=1280, out_features=15, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_loader\n",
    "\n",
    "\n",
    "\n",
    "def load_model(PATH = 'mov3.pkl'):\n",
    "    test_model = mobilenetv3(n_class = NUM_CLASSES,input_size=224, dropout=0.8, mode='small', width_mult=1.0)\n",
    "    test_model.load_state_dict(torch.load(PATH))\n",
    "    return test_model\n",
    "\n",
    "\n",
    "test_model = load_model('mov3.pkl')\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "test_model.to(DEVICE)\n",
    "if(DEVICE == 'cuda'):\n",
    "    test_model=torch.nn.DataParallel(test_model)\n",
    "\n",
    "test_model.eval() #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestDataSet & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading finished, 45000 totally\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_root = '',transform = None):\n",
    "        self.mean = 0.16443629118313927\n",
    "        self.std = 0.3239202530511322\n",
    "        self.transform = transform\n",
    "        self.label_mapper = {'airplane': 0, 'ant': 1, 'bear': 2, 'bird': 3, 'bridge': 4,\n",
    "                        'bus'     : 5, 'calendar': 6, 'car': 7, 'chair': 8, 'dog': 9,\n",
    "                        'dolphin' : 10, 'door': 11, 'flower': 12, 'fork': 13, 'truck': 14}\n",
    "        self.IO_mapper = []\n",
    "        self.picids = []\n",
    "        temp_list = os.listdir(data_root)\n",
    "        for img in temp_list:\n",
    "            self.picids.append(img.split('.')[0])\n",
    "            full_str = os.path.join(data_root,img)\n",
    "            \n",
    "            \n",
    "            self.IO_mapper.append(full_str)\n",
    "        print('Data loading finished,',len(self.IO_mapper),'totally')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.IO_mapper)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image = cv2.imread(self.IO_mapper[idx],0)\n",
    "        picid = self.picids[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(Image.fromarray(image))\n",
    "        return picid, image\n",
    "\n",
    "# Preprocessing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.16443629118313927, ),(0.3239202530511322, )),\n",
    "])\n",
    "test_root = './released_test'\n",
    "# Dataset & Dataloader\n",
    "TestD = TestDataset(test_root, transform = test_transform)\n",
    "Testdl = DataLoader(TestD, batch_size = 1024, shuffle = True, num_workers = 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do inference & writing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:10,  4.73it/s]\n",
      "100%|██████████| 45000/45000 [00:00<00:00, 866253.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000 45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "pics = []\n",
    "labels = []\n",
    "with torch.set_grad_enabled(False): \n",
    "    for i, (picids,features) in tqdm(enumerate(Testdl)):\n",
    "        features = features.to(DEVICE)\n",
    "        picids = list(picids)\n",
    "        logits, probas = test_model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        predicted_labels = list(predicted_labels.cpu().numpy())\n",
    "        pics += picids\n",
    "        labels += predicted_labels\n",
    "    print(len(pics),len(labels))\n",
    "    \n",
    "headers = ['id','categories']\n",
    "\n",
    "with open('019033910029.csv','w')as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "    for i in tqdm(range(0,len(pics))):\n",
    "        f_csv.writerow([int(pics[i]),int(labels[i])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
